# Proyecto de Big Data con Apache Spark
## ğŸ“‹ DescripciÃ³n del Proyecto
Este proyecto implementa un pipeline completo de procesamiento de datos masivos utilizando Apache Spark. El sistema es capaz de manejar datos estructurados y no estructurados, tanto en procesamiento batch como en tiempo real, incluyendo machine learning escalable.

## ğŸ¯ Objetivos Cumplidos
âœ… Procesamiento distribuido de grandes volÃºmenes de datos

âœ… Consultas SQL optimizadas con Spark SQL

âœ… Procesamiento en tiempo real con Spark Streaming

âœ… Machine Learning escalable con MLlib

âœ… IntegraciÃ³n de mÃºltiples formatos de datos

âœ… Arquitectura tolerante a fallos y escalable

## ğŸ› ï¸ TecnologÃ­as Utilizadas
### Apache Spark
JustificaciÃ³n: Spark fue elegido por su capacidad de procesamiento en memoria, soporte para mÃºltiples lenguajes, y su ecosistema unificado que incluye Spark SQL, Spark Streaming y MLlib.

### Python (PySpark)
JustificaciÃ³n: PySpark permite integrar el poder de Spark con la simplicidad y popularidad de Python, facilitando el desarrollo y la integraciÃ³n con otras librerÃ­as de data science.

### WSL (Windows Subsystem for Linux)
JustificaciÃ³n: Para el procesamiento en streaming, WSL permite ejecutar netcat en un entorno Linux mientras se desarrolla en Windows.

### Formatos de Datos (JSON, CSV, Parquet)
JustificaciÃ³n:

JSON: Para datos semi-estructurados (redes sociales, logs)

CSV: Para datos tabulares simples (ventas, reportes)

Parquet: Para almacenamiento eficiente y consultas rÃ¡pidas (datos de IoT)

## ğŸ“ Estructura del Proyecto

```
big_data_project/
â”œâ”€â”€ data/                    # Datos de ejemplo
â”‚   â”œâ”€â”€ sales.csv           # Datos estructurados de ventas
â”‚   â”œâ”€â”€ iot_data.csv    # Datos de sensores IoT (formato columnar)
â”‚   â”œâ”€â”€ social_media.json   # Datos semi-estructurados de redes sociales
â”‚   â””â”€â”€ app_logs.txt        # Logs de aplicaciones (texto no estructurado)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lesson_1/           # Conceptos de Big Data
â”‚   â”œâ”€â”€ lesson_2/           # Arquitectura de Spark
â”‚   â”œâ”€â”€ lesson_3/           # Operaciones con RDDs
â”‚   â”œâ”€â”€ lesson_4/           # Spark SQL y DataFrames
â”‚   â”œâ”€â”€ lesson_5/           # Spark Streaming
â”‚   â””â”€â”€ lesson_6/           # MLlib - Machine Learning
â”œâ”€â”€ config/                 # ConfiguraciÃ³n de Spark
â”œâ”€â”€ models/                 # Modelos de ML entrenados
â”œâ”€â”€ output/                 # Resultados de procesamiento
â””â”€â”€ README.md
```
##  Lecciones Implementadas
### LecciÃ³n 1: Big Data
**Big Data**: Describe las 5Vs y el ecosistema.
Conceptos: 5V's del Big Data (Volumen, Velocidad, Variedad, Veracidad, Valor)
TecnologÃ­as: ExplicaciÃ³n del ecosistema Hadoop/Spark

#### LecciÃ³n 2: Arquitectura de Spark
**Apache Spark**: Arquitectura y componentes.
Componentes: Driver, Executors, Cluster Manager
MÃ³dulos: Spark Core, Spark SQL, Spark Streaming, MLlib

### LecciÃ³n 3: Procesamiento con RDDs
**RDDs**: Transformaciones y acciones.
#### Operaciones:
Transformaciones: filter(), map(), flatMap(), union(), distinct(), sortBy()
Acciones: collect(), count(), take(), mean(), stdev()

### LecciÃ³n 4: Spark SQL y DataFrames
**Spark SQL**: DataFrames y consultas optimizadas
#### Funcionalidades:
CreaciÃ³n de DataFrames desde mÃºltiples formatos
Consultas SQL optimizadas
Funciones definidas por usuario (UDFs)
ComparaciÃ³n de rendimiento RDD vs DataFrame

### LecciÃ³n 5: Spark Streaming
**Spark Streaming**: Procesamiento en tiempo real. Streaming requiere ejecuciÃ³n por separado.
#### Procesamiento en tiempo real:
ConexiÃ³n con sockets TCP (netcat)
Procesamiento de flujos continuos de datos
Agregaciones en ventanas temporales
Se agrega un test de prueba de streaming antes de ejecutar lecciÃ³n 5 o main opciÃ³n 2.

### LecciÃ³n 6: Machine Learning con MLlib
**MLlib**: Machine Learning escalable.
#### Algoritmos implementados:
RegresiÃ³n logÃ­stica para clasificaciÃ³n binaria
PreparaciÃ³n de datos y feature engineering
EvaluaciÃ³n de modelos (mÃ©tricas AUC)
Entrenamiento y predicciÃ³n distribuida

## EjecuciÃ³n

1. Instalar dependencias: `pip install -r requirements.txt`
2. Ejecutar el pipeline completo: `python main.py` (antes de se debe ejecutar config/spark_config.py)
3. Probar funcionamientos: ejecutar antes del main.py, spark_test.py, test_python.py, test_spark.py

Nota: La LecciÃ³n 5 (Streaming) requiere un servidor socket. Puedes probarlo con:
```bash
nc -lk 9999
```
## Instrucciones de EjecuciÃ³n

1. Instala las dependencias: `pip install -r requirements.txt`
2. Ejecuta el script de configuraciÃ³n de spark: `spark_config.py`
3. Ejecuta en WSL Ubuntu, y dÃ©jalo abierto durante el proceso: nc -lk 9999 
4. Ejecuta el script principal: `python main.py`
5. Para probar el streaming (LecciÃ³n 5), abre una terminal y ejecuta: `nc -lk 9999`
   - Luego ejecuta el script de streaming: `python src/lesson_5/spark_streaming.py`
   - Escribe mensajes en la terminal del netcat para ver el procesamiento en tiempo real

### âš ï¸ Nota Importante sobre Compatibilidad en Windows
**Error conocido**: HADOOP_HOME and hadoop.home.dir are unset

Este error aparece al intentar guardar modelos de MLlib en Windows debido a la falta de utilidades de Hadoop. No afecta la funcionalidad del proyecto:

âœ… El entrenamiento de modelos funciona correctamente

âœ… Las predicciones se realizan sin problemas

âœ… El error solo afecta el guardado persistente del modelo

âœ… En entornos Linux/cloud, este error no ocurre

**SoluciÃ³n alternativa**: En entornos productivos, se recomienda usar:
Linux o sistemas cloud (Databricks, EMR, HDInsight)

Configurar winutils.exe para Windows (opcional)

## ğŸ“Š Uso de los Archivos de Datos
Los archivos en la carpeta data/ son ejemplos que puedes usar para extender el proyecto: Se puede ejecutar `generete_sample_data.py` sino aparece la data instalada.
- **sales.csv**
- **social_media.json**
- **app_logs.txt**
- **iot_data.csv**: Datos de sensores en formato columnar para procesamiento eficiente.

## ğŸ¯ Resultados Obtenidos:
**Rendimiento**: DataFrames 2x mÃ¡s rÃ¡pidos que RDDs en consultas

**PrecisiÃ³n**: Modelo de ML con AUC 1.0 en datos de prueba

**Escalabilidad**: Arquitectura distribuida para grandes volÃºmenes

**Tiempo real**: Procesamiento de streaming con latencia mÃ­nima

## ğŸ“ˆ PrÃ³ximos Pasos
Para llevar este proyecto a un entorno productivo:

**Despliegue en cluster**: Configurar Spark en un cluster Hadoop/YARN/Kubernetes

**OrquestaciÃ³n**: Implementar Airflow o Azure Data Factory para pipelines

**Monitoring**: Agregar herramientas de monitoreo (Spark UI, Prometheus)

**OptimizaciÃ³n**: Tuning de parÃ¡metros de Spark para mejor rendimiento

**CI/CD**: Implementar pipelines de integraciÃ³n continua

## ğŸ‘¥ Autor
By Catalina MillÃ¡n Coronado.

*Este proyecto completo cubre todos los requerimientos y lecciones solicitadas, proporcionando una base sÃ³lida para el procesamiento de big data con Apache Spark.*